{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1248d8bc-95b7-4981-985c-c935e3c2244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408c0b3a-ecea-4135-8a06-5d15a9314875",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = pathlib.Path(\"../../data\")\n",
    "INPUTS_DIR = DATA_ROOT / \"input\"\n",
    "DATASET_DIR = INPUTS_DIR / \"mayo-clinic-strip-ai\"\n",
    "\n",
    "TRAIN_TIFF_DIR = DATASET_DIR / \"train\"\n",
    "TEST_TIFF_DIR = DATASET_DIR / \"test\"\n",
    "OUTPUT_DIR = DATA_ROOT / \"working\"\n",
    "EMBEDDINGS_DIR = INPUTS_DIR / \"embs\"\n",
    "FG_IDS_DIR = INPUTS_DIR / \"fg_ids\"\n",
    "DEVICE = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "EMB_SIZE = 512\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63dfe35-11fa-4f83-afab-5269820c8fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.read_csv(DATASET_DIR / \"train.csv\")\n",
    "\n",
    "labels_dict = {\"CE\": 0, \"LAA\": 1}\n",
    "\n",
    "def label_to_int(x):\n",
    "    return labels_dict[x[\"label\"]]\n",
    "input_df[\"label_int\"] = input_df.apply(label_to_int, axis=1)\n",
    "\n",
    "slide_paths = [TRAIN_TIFF_DIR / (x + \".tif\" ) for x in input_df[\"image_id\"]]\n",
    "foreground_ids_dir = OUTPUT_DIR / \"fg_ids\"\n",
    "downscaled_dir = OUTPUT_DIR / \"downscaled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37983cc9-13d3-4776-9cb8-aedef032d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "slide_ids = []\n",
    "for path in TRAIN_TIFF_DIR.glob(\"*.tif\"):\n",
    "    slide_ids.append(\".\".join(path.name.split(\".\")[:-1]))\n",
    "\n",
    "input_df = input_df[input_df[\"image_id\"].isin(slide_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9470c27e-50b5-439c-ad6e-6eef105e3d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = []\n",
    "for _, sample in input_df.iterrows():\n",
    "    embs.append(np.load(EMBEDDINGS_DIR / (sample.image_id + \".npy\")))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77f35a0f-7ceb-484b-8b4b-075bf1646acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3726, 230, 130, 8341, 425]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape[0] for x in embs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "e80f8552-f699-4324-a112-f4fc7c3e37a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoarseDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe, embs_dir, seq_length=0, pad_emb=None):\n",
    "        super().__init__()\n",
    "        self.dataframe = dataframe\n",
    "        self.embs_dir = embs_dir\n",
    "        self.pad_emb = pad_emb       \n",
    "        self.embs_paths = [embs_dir / (x + \".npy\" )for x in dataframe[\"image_id\"]]\n",
    "        self.n_embs = len(self.embs_paths)\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def get_seq_length(self):\n",
    "        max_l = 0\n",
    "        for embs_path in self.embs_paths:\n",
    "            embs = np.load(embs_path, allow_pickle=False)\n",
    "            if max_l < embs.shape[0]:\n",
    "                max_l = embs.shape[0]        \n",
    "        return max_l\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_embs\n",
    "    \n",
    "    def embs_to_length(self, embs, to_length, pad_vector=None):\n",
    "        emb_length, emb_size = embs.shape\n",
    "        if pad_vector is None:\n",
    "            pad_vector = torch.zeros(emb_size)\n",
    "\n",
    "        pad_size = to_length - emb_length\n",
    "        if pad_size > 0:\n",
    "            padding = torch.stack([pad_vector.T]*pad_size)\n",
    "            embs = torch.concat([embs, padding], 0)\n",
    "\n",
    "        return embs[0:to_length,:]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        embs = np.load(self.embs_paths[idx], allow_pickle=False)\n",
    "        embs = torch.from_numpy(embs)\n",
    "        label = train_df.iloc[idx][\"label_int\"]\n",
    "        return {\"embs\": embs, \"label\": label}\n",
    "    \n",
    "    def collate_fn(self, samples):\n",
    "        max_length = np.max([x[\"embs\"].shape[0] for x in samples])\n",
    "        labels = torch.tensor([x[\"label\"] for x in samples], dtype=torch.long)\n",
    "        in_batch = torch.stack([self.embs_to_length(x[\"embs\"], max_length) for x in samples])\n",
    "        return {\"embs\": in_batch, \"label\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e26b73-4cbd-448d-8b22-daa2c37177af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "4837eaef-2b3e-4ce1-a793-ef01ab0168ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_slides = []\n",
    "norms = []\n",
    "for path in EMBEDDINGS_DIR.glob(\"*npy\"):\n",
    "    embs = np.load(path, allow_pickle=False)\n",
    "    embs_slides.append(embs.shape[0])\n",
    "    norms.append(np.sqrt(np.sum(np.square(embs), 1)))\n",
    "\n",
    "norms = np.concatenate(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "9a9ed0fb-3193-4f49-be39-a87e1b1b2e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12852,)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "da135b3e-3c23-42d6-92d6-e0469b257874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322.6744"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "45403c14-db7f-4028-bb3b-932e7a4ace8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 70 artists>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(norms, bins=70)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "9f17f6b7-3606-4e21-b135-ab65c30cf15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "908d7bbe-0953-4821-8e98-6cc603b271e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoarseTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,emb_size, nhead, num_layers, num_classes=2, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=emb_size, \n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward, \n",
    "            batch_first=True\n",
    "        )\n",
    "        self.feature_extractor = nn.TransformerEncoder(\n",
    "            encoder_layer=enc_layer, \n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.pooler = nn.AdaptiveAvgPool1d(1)\n",
    "        self.classifier = nn.Linear(emb_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.permute(0,2,1).contiguous()\n",
    "        x = self.pooler(x).squeeze().contiguous()\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "coarse_model = CoarseTransformer(512, nhead=4, num_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "f5f1a004-f923-403b-b2ed-414838436262",
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=2, shuffle=True)\n",
    "train_folds = []\n",
    "val_folds = []\n",
    "for train_ids, val_ids in  skf.split(input_df, input_df.label_int):\n",
    "    train_folds.append(train_ids)\n",
    "    val_folds.append(val_ids)\n",
    "    \n",
    "split_id = 0\n",
    "train_ids = train_folds[split_id]\n",
    "val_ids = val_folds[split_id]\n",
    "train_df = input_df.iloc[train_ids]\n",
    "val_df = input_df.iloc[val_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "3b493293-934b-4c8e-8208-ee4b650a7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 10\n",
    "batch_size = 128\n",
    "train_dataset = CoarseDataset(train_df, EMBEDDINGS_DIR, seq_length=seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=train_dataset.collate_fn)\n",
    "\n",
    "val_dataset = CoarseDataset(val_df, EMBEDDINGS_DIR, seq_length=seq_length)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=val_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "9c0716c7-a74a-4219-91d9-cbd8fdc3da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CoarseDataset(input_df, EMBEDDINGS_DIR, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0806c6ad-2042-4d12-8eca-7d4fc860b8d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "817007c5-17cb-4fc0-9e36-2db52e1cefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "1d0e5409-ec5c-48d1-9d09-52bb51997cf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"label\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "d9800639-7db7-4d77-a2ac-0fcbff59761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHTS = (1,1)\n",
    "optimiser = torch.optim.Adam(coarse_model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(WEIGHTS, dtype=torch.float))\n",
    "\n",
    "for epoch_i in range(NUM_EPOCHS):\n",
    "    for batch in train_loader:\n",
    "        outputs = coarse_model(batch[\"embs\"])\n",
    "        loss = criterion(outputs, batch[\"label\"])\n",
    "        loss.backward()\n",
    "        optimiser.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "cf46f2cd-8ba6-421b-a85d-48e48d6ccf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batch[\"label\"].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "8fcdca0f-d03f-434c-8a31-6c2f21760a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossEntropyLoss()"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "4c205184-f194-4c04-bc0d-07390137921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(outputs, batch[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a852e-5b5a-461c-96c0-4f5ce931c18e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "118ead8d-a663-423f-9480-049ea398f407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 230, 512])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "331e8e39-2ffb-4ca5-9a4d-173d7b0b3175",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1189770672.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [157]\u001b[0;36m\u001b[0m\n\u001b[0;31m    .shape\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c86d4ca-6a89-431a-b047-7c8122e66f53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
